{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cluciani-angel/documentation/blob/main/Casino_Data_ETL_Script_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import uuid\n",
        "import os\n",
        "import logging\n",
        "from io import StringIO\n",
        "\n",
        "# --- 1. Configuration & Logging Setup ---\n",
        "\n",
        "# Set up basic logging to capture informational messages and errors.\n",
        "# This will create a log file in the same directory as the script.\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"etl_process.log\"),\n",
        "        logging.StreamHandler() # Also print logs to the console\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- 2. Helper Function to Safely Convert IDs ---\n",
        "\n",
        "def safe_hex_to_uuid(hex_string):\n",
        "    \"\"\"\n",
        "    Safely converts a hex string to a UUID object.\n",
        "\n",
        "    This function is the core solution to the \"Got bytestring\" error.\n",
        "    It validates the input string before trying to convert it.\n",
        "\n",
        "    Args:\n",
        "        hex_string (str): The string to convert, expected to be 32 hex characters.\n",
        "\n",
        "    Returns:\n",
        "        uuid.UUID: The UUID object if conversion is successful.\n",
        "        pd.NA: Returns pandas' Not Available marker if the string is invalid.\n",
        "    \"\"\"\n",
        "    if not isinstance(hex_string, str) or len(hex_string) != 32:\n",
        "        # The bytestring must be 16 bytes, which means the hex string must be 32 characters.\n",
        "        # This check catches the length error before it happens.\n",
        "        return pd.NA\n",
        "    try:\n",
        "        # Convert the 32-character hex string into a 16-byte object, then into a UUID.\n",
        "        return uuid.UUID(bytes=bytes.fromhex(hex_string))\n",
        "    except (ValueError, TypeError):\n",
        "        # Catches other errors, like non-hex characters in the string.\n",
        "        return pd.NA\n",
        "\n",
        "# --- 3. The ETL Pipeline Functions ---\n",
        "\n",
        "def extract_data(file_path):\n",
        "    \"\"\"\n",
        "    Extracts data from a given CSV file path.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Starting data extraction from: {file_path}\")\n",
        "    if not os.path.exists(file_path):\n",
        "        logging.error(f\"File not found at {file_path}. Creating a dummy dataframe for demonstration.\")\n",
        "        # Create a dummy CSV in memory to demonstrate the script's functionality\n",
        "        dummy_csv_data = \"\"\"id_hex,player_name,transaction_amount,timestamp\n",
        "d3d6e0a01b3a4c9f8f1e7d6a5b4c3d2e,Player1,100,2025-08-10T10:00:00Z\n",
        "636f727275707465,Player2,250,2025-_08-10T11:30:00Z\n",
        "e1c2a3b45d6e7f8a9b0c1d2e3f4a5b6c,Player3,50,2025-08-10T12:15:00Z\n",
        "invalid-hex-string,Player4,120,2025-08-10T13:00:00Z\n",
        "d3d6e0a01b3a4c9f8f1e7d6a5b4c3d2f,Player5,300,2025-08-10T14:00:00Z\n",
        "\"\"\"\n",
        "        return pd.read_csv(StringIO(dummy_csv_data), dtype=str)\n",
        "\n",
        "    try:\n",
        "        # Reading all columns as strings initially prevents pandas from making wrong type guesses.\n",
        "        df = pd.read_csv(file_path, dtype=str, sep=';', engine='python')\n",
        "        logging.info(\"Data extraction successful.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to read data from {file_path}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def transform_data(df, id_column_name):\n",
        "    \"\"\"\n",
        "    Cleans, validates, and transforms the raw dataframe.\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    logging.info(\"Starting data transformation...\")\n",
        "\n",
        "    df_transformed = df.copy()\n",
        "\n",
        "    # --- NEW DEBUGGING STEP ---\n",
        "    # Log the columns that were actually found in the file. This helps diagnose mismatches.\n",
        "    logging.info(f\"Columns found in the CSV file: {df_transformed.columns.to_list()}\")\n",
        "    # --- END NEW DEBUGGING STEP ---\n",
        "\n",
        "    # Check if the required ID column exists\n",
        "    if id_column_name not in df_transformed.columns:\n",
        "        logging.error(f\"The specified ID column '{id_column_name}' was not found in the data. Please check the column list above and update the 'ID_COLUMN_TO_VALIDATE' variable in the script.\")\n",
        "        return None\n",
        "\n",
        "    # Apply the safe UUID conversion function\n",
        "    logging.info(f\"Converting '{id_column_name}' to UUID. Invalid entries will be flagged.\")\n",
        "    df_transformed['uuid'] = df_transformed[id_column_name].apply(safe_hex_to_uuid)\n",
        "\n",
        "    invalid_rows = df_transformed[df_transformed['uuid'].isna()]\n",
        "    if not invalid_rows.empty:\n",
        "        logging.warning(f\"Found {len(invalid_rows)} rows with invalid IDs. See details below.\")\n",
        "        for index, row in invalid_rows.iterrows():\n",
        "            logging.warning(f\"  - Row Index {index}: Invalid ID value = '{row[id_column_name]}'\")\n",
        "\n",
        "    logging.info(\"Standardizing column names and types.\")\n",
        "\n",
        "    logging.info(\"Data transformation complete.\")\n",
        "    return df_transformed\n",
        "\n",
        "def load_data(df, output_path):\n",
        "    \"\"\"\n",
        "    Saves the transformed dataframe to a new CSV file.\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        logging.warning(\"No data to load; skipping file creation.\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"Loading transformed data to: {output_path}\")\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        df.to_csv(output_path, index=False)\n",
        "        logging.info(\"Data loading successful.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save data to {output_path}. Error: {e}\")\n",
        "\n",
        "# --- 4. Main Execution Block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(\"====== Starting Casino Data ETL Process ======\")\n",
        "\n",
        "    # --- Configuration: SET YOUR FILE PATHS AND COLUMN NAME HERE ---\n",
        "\n",
        "    INPUT_CSV_PATH = '/content/drive/MyDrive/Reportes Auditoria/SIELCON/08-2025/Tickets08-2025.csv'\n",
        "\n",
        "    # IMPORTANT: Change this to your actual column name.\n",
        "    # Look at the log output from the line \"Columns found in the CSV file:\" to find the correct name.\n",
        "    ID_COLUMN_TO_VALIDATE = 'id_hex'\n",
        "\n",
        "    OUTPUT_CSV_PATH = 'processed/tickets_cleaned.csv'\n",
        "\n",
        "    # --- Run the Pipeline ---\n",
        "    raw_dataframe = extract_data(INPUT_CSV_PATH)\n",
        "    transformed_dataframe = transform_data(raw_dataframe, ID_COLUMN_TO_VALIDATE)\n",
        "    load_data(transformed_dataframe, OUTPUT_CSV_PATH)\n",
        "\n",
        "    logging.info(\"====== ETL Process Finished ======\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "znJZhKwzjhLf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}